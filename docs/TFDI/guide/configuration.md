# âš™ï¸ TFDI å¯¼èˆªæ•°æ®è½¬æ¢å™¨é…ç½®æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»äº† TFDI å¯¼èˆªæ•°æ®è½¬æ¢å™¨çš„å„ç§é…ç½®é€‰é¡¹ï¼Œå¸®åŠ©æ‚¨æ ¹æ®éœ€æ±‚ä¼˜åŒ–è½¬æ¢è¿‡ç¨‹å’Œè¾“å‡ºç»“æœã€‚

## ğŸ¯ é…ç½®æ¦‚è§ˆ

TFDI è½¬æ¢å™¨é‡‡ç”¨çµæ´»çš„é…ç½®ç³»ç»Ÿï¼Œæ”¯æŒï¼š
- **ğŸ“ è¾“å‡ºé…ç½®** - è‡ªå®šä¹‰è¾“å‡ºç›®å½•å’Œæ–‡ä»¶æ ¼å¼
- **ğŸ“Š æ•°æ®å¤„ç†å‚æ•°** - è°ƒæ•´åæ ‡ç²¾åº¦å’Œæ•°æ®è¿‡æ»¤
- **âš¡ æ€§èƒ½ä¼˜åŒ–** - å†…å­˜ç®¡ç†å’Œå¤„ç†é€Ÿåº¦è°ƒä¼˜
- **ğŸ” éªŒè¯é€‰é¡¹** - æ•°æ®å®Œæ•´æ€§å’Œè´¨é‡æ§åˆ¶è®¾ç½®

## ğŸ“‹ é…ç½®æ–¹å¼

### 1. é»˜è®¤é…ç½® (æ¨èæ–°æ‰‹)
```python
# ä½¿ç”¨é»˜è®¤é…ç½®è¿è¡Œ
python Fenix2TFDINavDataConverter.py
# è½¬æ¢å™¨å°†ä½¿ç”¨é¢„è®¾çš„æœ€ä½³é…ç½®
```

### 2. ä»£ç å†…é…ç½®
```python
from dataclasses import dataclass

@dataclass
class ConverterConfig:
    """è½¬æ¢å™¨é…ç½®ç±»"""
    output_dir: str = "Primary"
    procedure_legs_dir: str = "Primary/ProcedureLegs" 
    archive_name: str = "Primary.7z"
    coordinate_precision: int = 8
    vnav_threshold: float = 2.5
```

### 3. äº¤äº’å¼é…ç½®
```python
# è½¬æ¢å™¨è¿è¡Œæ—¶çš„äº¤äº’å¼é…ç½®
# ç”¨æˆ·å¯ä»¥åœ¨è¿è¡Œè¿‡ç¨‹ä¸­è®¾ç½®å…³é”®å‚æ•°
```

## ğŸ”§ æ ¸å¿ƒé…ç½®é€‰é¡¹

### è¾“å‡ºè·¯å¾„é…ç½®

#### è¾“å‡ºç›®å½•è®¾ç½®
**å‚æ•°å**: `output_dir`  
**é»˜è®¤å€¼**: `"Primary"`  
**æè¿°**: æ‰€æœ‰ JSON æ–‡ä»¶çš„è¾“å‡ºç›®å½•

**ä½¿ç”¨ç¤ºä¾‹:**
```python
config = ConverterConfig(
    output_dir="TFDI_NavData",  # è‡ªå®šä¹‰è¾“å‡ºç›®å½•å
)
```

**ç›®å½•ç»“æ„:**
```
TFDI_NavData/           # ä¸»è¾“å‡ºç›®å½•
â”œâ”€â”€ Airports.json       # æœºåœºæ•°æ®
â”œâ”€â”€ Runways.json        # è·‘é“æ•°æ®
â”œâ”€â”€ Waypoints.json      # èˆªè·¯ç‚¹æ•°æ®
â”œâ”€â”€ ...                 # å…¶ä»– JSON æ–‡ä»¶
â””â”€â”€ ProcedureLegs/      # ç¨‹åºæ®µå­ç›®å½•
    â”œâ”€â”€ TermID_1.json
    â”œâ”€â”€ TermID_2.json
    â””â”€â”€ ...
```

#### ç¨‹åºæ®µç›®å½•
**å‚æ•°å**: `procedure_legs_dir`  
**é»˜è®¤å€¼**: `"Primary/ProcedureLegs"`  
**æè¿°**: ç»ˆç«¯ç¨‹åºæ®µæ–‡ä»¶çš„è¾“å‡ºç›®å½•

**é…ç½®ç¤ºä¾‹:**
```python
config = ConverterConfig(
    output_dir="NavData",
    procedure_legs_dir="NavData/Procedures",  # è‡ªå®šä¹‰ç¨‹åºæ®µç›®å½•
)
```

#### å‹ç¼©åŒ…åç§°
**å‚æ•°å**: `archive_name`  
**é»˜è®¤å€¼**: `"Primary.7z"`  
**æè¿°**: æœ€ç»ˆç”Ÿæˆçš„å‹ç¼©åŒ…æ–‡ä»¶å

**å‘½åè§„èŒƒ:**
```python
# åŒ…å«æ—¶é—´æˆ³çš„å‘½å
import datetime
now = datetime.datetime.now()
config = ConverterConfig(
    archive_name=f"TFDI_NavData_{now.strftime('%Y%m%d_%H%M')}.7z"
)

# åŒ…å«ç‰ˆæœ¬ä¿¡æ¯çš„å‘½å
config = ConverterConfig(
    archive_name="TFDI_NavData_v1.0.7z"
)
```

### æ•°æ®å¤„ç†é…ç½®

#### åæ ‡ç²¾åº¦è®¾ç½®
**å‚æ•°å**: `coordinate_precision`  
**é»˜è®¤å€¼**: `8`  
**èŒƒå›´**: `4 - 12`  
**æè¿°**: åœ°ç†åæ ‡çš„å°æ•°ç‚¹ä½æ•°

**ç²¾åº¦å¯¹æ¯”è¡¨:**
| ç²¾åº¦ | è¯¯å·®èŒƒå›´ | é€‚ç”¨åœºæ™¯ | æ–‡ä»¶å¤§å°å½±å“ |
|------|----------|----------|--------------|
| 4 ä½ | ~11 ç±³ | åŸºç¡€å¯¼èˆª | æœ€å° |
| 6 ä½ | ~1.1 ç±³ | æ ‡å‡†å¯¼èˆª | å° |
| 8 ä½ | ~1.1 å˜ç±³ | é«˜ç²¾åº¦å¯¼èˆª | é»˜è®¤ |
| 10 ä½ | ~1.1 æ¯«ç±³ | æé«˜ç²¾åº¦ | å¤§ |
| 12 ä½ | ~0.1 æ¯«ç±³ | æµ‹é‡çº§ç²¾åº¦ | æœ€å¤§ |

**é…ç½®ç¤ºä¾‹:**
```python
# é«˜ç²¾åº¦é…ç½® (æ¨è)
config = ConverterConfig(coordinate_precision=8)

# å¹³è¡¡é…ç½® (æ–‡ä»¶æ›´å°)
config = ConverterConfig(coordinate_precision=6)

# æé«˜ç²¾åº¦é…ç½® (æ–‡ä»¶æ›´å¤§)
config = ConverterConfig(coordinate_precision=10)
```

#### VNAV é˜ˆå€¼è®¾ç½®
**å‚æ•°å**: `vnav_threshold`  
**é»˜è®¤å€¼**: `2.5`  
**å•ä½**: åº¦  
**æè¿°**: FAF ç‚¹æ£€æµ‹çš„ VNAV è§’åº¦é˜ˆå€¼

**é˜ˆå€¼å«ä¹‰:**
```python
# ä¸¥æ ¼çš„ FAF æ£€æµ‹ (æ›´å°‘ä½†æ›´å‡†ç¡®çš„ FAF ç‚¹)
config = ConverterConfig(vnav_threshold=2.0)

# æ ‡å‡† FAF æ£€æµ‹ (å¹³è¡¡å‡†ç¡®æ€§å’Œè¦†ç›–ç‡)
config = ConverterConfig(vnav_threshold=2.5)

# å®½æ¾çš„ FAF æ£€æµ‹ (æ›´å¤š FAF ç‚¹ï¼Œå¯èƒ½åŒ…å«è¯¯æ£€)
config = ConverterConfig(vnav_threshold=3.0)
```

**FAF æ£€æµ‹é€»è¾‘:**
```python
def is_faf_point(vnav_angle: float, vnav_threshold: float) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸º FAF ç‚¹"""
    return (vnav_angle is not None and 
            vnav_angle <= vnav_threshold and 
            vnav_angle > 0)
```

## ğŸš€ æ€§èƒ½é…ç½®

### SQLite æ•°æ®åº“ä¼˜åŒ–

#### æ•°æ®åº“è¿æ¥è®¾ç½®
```python
# SQLite æ€§èƒ½ä¼˜åŒ–é…ç½®
sqlite_pragmas = {
    "journal_mode": "WAL",          # å†™å‰æ—¥å¿—æ¨¡å¼
    "synchronous": "NORMAL",        # å¹³è¡¡æ€§èƒ½å’Œå®‰å…¨æ€§
    "cache_size": "10000",          # ç¼“å­˜é¡µæ•° (çº¦ 40MB)
    "temp_store": "MEMORY",         # ä¸´æ—¶æ•°æ®å­˜å‚¨åœ¨å†…å­˜
    "mmap_size": "268435456",       # å†…å­˜æ˜ å°„å¤§å° (256MB)
}

def optimize_database_connection(conn):
    """ä¼˜åŒ–æ•°æ®åº“è¿æ¥"""
    for pragma, value in sqlite_pragmas.items():
        conn.execute(f"PRAGMA {pragma}={value}")
```

#### æ‰¹å¤„ç†è®¾ç½®
**å‚æ•°**: æ‰¹å¤„ç†å¤§å°  
**é»˜è®¤å€¼**: `1000`  
**æè¿°**: å•æ¬¡å¤„ç†çš„è®°å½•æ•°é‡

**é…ç½®ç­–ç•¥:**
```python
import psutil

def get_optimal_batch_size():
    """æ ¹æ®ç³»ç»Ÿå†…å­˜è‡ªåŠ¨è°ƒæ•´æ‰¹å¤„ç†å¤§å°"""
    available_memory_gb = psutil.virtual_memory().available / (1024**3)
    
    if available_memory_gb < 4:
        return 500      # ä½å†…å­˜ç³»ç»Ÿ
    elif available_memory_gb < 8:
        return 1000     # æ ‡å‡†é…ç½®
    else:
        return 2000     # é«˜å†…å­˜ç³»ç»Ÿ

# ä½¿ç”¨ç¤ºä¾‹
batch_size = get_optimal_batch_size()
```

### å†…å­˜ç®¡ç†é…ç½®

#### å†…å­˜ç›‘æ§è®¾ç½®
```python
class MemoryMonitor:
    """å†…å­˜ç›‘æ§é…ç½®"""
    
    def __init__(self):
        self.memory_limit_gb = 6        # å†…å­˜ä½¿ç”¨é™åˆ¶
        self.warning_threshold = 0.8    # è­¦å‘Šé˜ˆå€¼ (80%)
        self.critical_threshold = 0.9   # å±é™©é˜ˆå€¼ (90%)
    
    def check_memory_usage(self):
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory = psutil.virtual_memory()
        usage_ratio = memory.used / memory.total
        
        if usage_ratio > self.critical_threshold:
            return "CRITICAL"
        elif usage_ratio > self.warning_threshold:
            return "WARNING"
        else:
            return "NORMAL"
```

#### åƒåœ¾å›æ”¶é…ç½®
```python
import gc

def configure_garbage_collection():
    """é…ç½®åƒåœ¾å›æ”¶"""
    # è®¾ç½®åƒåœ¾å›æ”¶é˜ˆå€¼
    gc.set_threshold(700, 10, 10)
    
    # å¯ç”¨åƒåœ¾å›æ”¶è°ƒè¯• (ä»…è°ƒè¯•æ—¶ä½¿ç”¨)
    # gc.set_debug(gc.DEBUG_STATS)

def force_cleanup():
    """å¼ºåˆ¶æ¸…ç†å†…å­˜"""
    gc.collect()
    gc.collect()  # è¿è¡Œä¸¤æ¬¡ç¡®ä¿å½»åº•æ¸…ç†
    gc.collect()
```

## ğŸ” éªŒè¯å’Œè´¨é‡æ§åˆ¶

### æ•°æ®éªŒè¯é…ç½®

#### éªŒè¯çº§åˆ«è®¾ç½®
```python
class ValidationConfig:
    """æ•°æ®éªŒè¯é…ç½®"""
    
    def __init__(self, level="standard"):
        self.level = level
        self.checks = self._get_checks_for_level(level)
    
    def _get_checks_for_level(self, level):
        """æ ¹æ®çº§åˆ«è·å–æ£€æŸ¥é¡¹ç›®"""
        base_checks = [
            "file_exists",
            "database_format", 
            "required_tables"
        ]
        
        if level == "basic":
            return base_checks
        
        elif level == "standard":
            return base_checks + [
                "coordinate_ranges",
                "data_types",
                "foreign_keys"
            ]
        
        elif level == "strict":
            return base_checks + [
                "coordinate_ranges",
                "data_types", 
                "foreign_keys",
                "data_consistency",
                "duplicate_detection",
                "business_rules"
            ]
```

#### åæ ‡éªŒè¯é…ç½®
```python
class CoordinateValidator:
    """åæ ‡éªŒè¯é…ç½®"""
    
    def __init__(self):
        # æœ‰æ•ˆåæ ‡èŒƒå›´
        self.lat_min = -90.0
        self.lat_max = 90.0
        self.lon_min = -180.0
        self.lon_max = 180.0
        
        # å¯ç–‘åæ ‡èŒƒå›´ (å¯èƒ½æ˜¯é”™è¯¯æ•°æ®)
        self.suspicious_zones = [
            {"lat": (0, 0), "lon": (0, 0)},      # åŸç‚¹åæ ‡å¯èƒ½æ˜¯é”™è¯¯
            {"lat": (90, 90), "lon": (0, 0)},    # æç‚¹åæ ‡éœ€è¦ç‰¹åˆ«æ£€æŸ¥
        ]
    
    def validate_coordinate(self, lat, lon):
        """éªŒè¯å•ä¸ªåæ ‡"""
        # æ£€æŸ¥åŸºæœ¬èŒƒå›´
        if not (self.lat_min <= lat <= self.lat_max):
            return False, f"çº¬åº¦è¶…å‡ºèŒƒå›´: {lat}"
        
        if not (self.lon_min <= lon <= self.lon_max):
            return False, f"ç»åº¦è¶…å‡ºèŒƒå›´: {lon}"
        
        # æ£€æŸ¥å¯ç–‘åæ ‡
        for zone in self.suspicious_zones:
            if (zone["lat"][0] <= lat <= zone["lat"][1] and 
                zone["lon"][0] <= lon <= zone["lon"][1]):
                return True, f"å¯ç–‘åæ ‡: ({lat}, {lon})"
        
        return True, "åæ ‡æœ‰æ•ˆ"
```

### è¾“å‡ºè´¨é‡æ§åˆ¶

#### æ–‡ä»¶æ ¼å¼éªŒè¯
```python
import json

class OutputValidator:
    """è¾“å‡ºæ–‡ä»¶éªŒè¯é…ç½®"""
    
    def __init__(self):
        self.required_files = [
            "Airports.json",
            "Runways.json", 
            "Waypoints.json",
            "Navaids.json",
            "Airways.json",
            "AirwayLegs.json",
            "Terminals.json",
            "ILSes.json"
        ]
        
        self.min_file_sizes = {
            "Airports.json": 1024,      # è‡³å°‘ 1KB
            "Waypoints.json": 10240,    # è‡³å°‘ 10KB
        }
    
    def validate_json_file(self, file_path):
        """éªŒè¯ JSON æ–‡ä»¶æ ¼å¼"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not isinstance(data, (dict, list)):
                return False, "JSON æ ¹å¯¹è±¡å¿…é¡»æ˜¯å­—å…¸æˆ–åˆ—è¡¨"
            
            if isinstance(data, dict) and len(data) == 0:
                return False, "JSON å¯¹è±¡ä¸ºç©º"
            
            return True, "JSON æ ¼å¼æœ‰æ•ˆ"
            
        except json.JSONDecodeError as e:
            return False, f"JSON æ ¼å¼é”™è¯¯: {e}"
        except Exception as e:
            return False, f"æ–‡ä»¶è¯»å–é”™è¯¯: {e}"
```

## ğŸ›ï¸ é«˜çº§é…ç½®

### æ—¥å¿—é…ç½®

#### æ—¥å¿—çº§åˆ«å’Œæ ¼å¼
```python
import logging
from rich.logging import RichHandler

class LoggingConfig:
    """æ—¥å¿—é…ç½®ç±»"""
    
    def __init__(self, level="INFO"):
        self.level = level
        self.format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        self.file_name = "tfdi_converter.log"
        self.max_file_size = 10 * 1024 * 1024  # 10MB
        self.backup_count = 3
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        # åˆ›å»º logger
        logger = logging.getLogger("TFDIConverter")
        logger.setLevel(getattr(logging, self.level))
        
        # Rich æ§åˆ¶å°å¤„ç†å™¨
        console_handler = RichHandler(rich_tracebacks=True)
        console_handler.setLevel(logging.INFO)
        console_formatter = logging.Formatter("%(message)s")
        console_handler.setFormatter(console_formatter)
        
        # æ–‡ä»¶å¤„ç†å™¨
        from logging.handlers import RotatingFileHandler
        file_handler = RotatingFileHandler(
            self.file_name, 
            maxBytes=self.max_file_size,
            backupCount=self.backup_count,
            encoding='utf-8'
        )
        file_handler.setLevel(logging.DEBUG)
        file_formatter = logging.Formatter(self.format)
        file_handler.setFormatter(file_formatter)
        
        # æ·»åŠ å¤„ç†å™¨
        logger.addHandler(console_handler)
        logger.addHandler(file_handler)
        
        return logger
```

### å‹ç¼©é…ç½®

#### å‹ç¼©çº§åˆ«è®¾ç½®
```python
import py7zr

class CompressionConfig:
    """å‹ç¼©é…ç½®ç±»"""
    
    def __init__(self):
        self.compression_level = 5      # å‹ç¼©çº§åˆ« (1-9)
        self.method = "LZMA2"          # å‹ç¼©ç®—æ³•
        self.solid = True              # å›ºå®å‹ç¼©
        self.multi_threading = True    # å¤šçº¿ç¨‹å‹ç¼©
    
    def create_archive(self, source_dir, archive_path):
        """åˆ›å»ºå‹ç¼©åŒ…"""
        filters = [
            {"id": py7zr.FILTER_LZMA2, "preset": self.compression_level}
        ]
        
        with py7zr.SevenZipFile(
            archive_path, 
            'w', 
            filters=filters,
            mp=self.multi_threading
        ) as archive:
            archive.writeall(source_dir, ".")
```

### è°ƒè¯•é…ç½®

#### è°ƒè¯•æ¨¡å¼è®¾ç½®
```python
class DebugConfig:
    """è°ƒè¯•é…ç½®ç±»"""
    
    def __init__(self, debug_mode=False):
        self.debug_mode = debug_mode
        self.save_intermediate_files = debug_mode
        self.verbose_logging = debug_mode
        self.performance_profiling = debug_mode
        self.memory_tracking = debug_mode
    
    def enable_debug_features(self):
        """å¯ç”¨è°ƒè¯•åŠŸèƒ½"""
        if self.debug_mode:
            # å¯ç”¨è¯¦ç»†æ—¥å¿—
            logging.getLogger().setLevel(logging.DEBUG)
            
            # å¯ç”¨æ€§èƒ½åˆ†æ
            if self.performance_profiling:
                import cProfile
                self.profiler = cProfile.Profile()
                self.profiler.enable()
            
            # å¯ç”¨å†…å­˜è·Ÿè¸ª
            if self.memory_tracking:
                import tracemalloc
                tracemalloc.start()
```

## ğŸ“ å®Œæ•´é…ç½®ç¤ºä¾‹

### åŸºç¡€é…ç½®ç¤ºä¾‹
```python
# é€‚åˆæ–°æ‰‹çš„ç®€å•é…ç½®
config = ConverterConfig(
    output_dir="TFDI_Output",
    coordinate_precision=8,
    vnav_threshold=2.5
)
```

### é«˜æ€§èƒ½é…ç½®ç¤ºä¾‹
```python
# é€‚åˆé«˜ç«¯ç¡¬ä»¶çš„æ€§èƒ½ä¼˜åŒ–é…ç½®
config = ConverterConfig(
    output_dir="Primary_HighPerf",
    coordinate_precision=8,
    vnav_threshold=2.5
)

# é…åˆæ€§èƒ½ä¼˜åŒ–è®¾ç½®
batch_size = 2000
memory_limit_gb = 8
enable_multi_threading = True
```

### é«˜è´¨é‡é…ç½®ç¤ºä¾‹
```python
# é€‚åˆå¯¹æ•°æ®è´¨é‡è¦æ±‚æé«˜çš„åœºæ™¯
config = ConverterConfig(
    output_dir="Primary_HighQuality", 
    coordinate_precision=10,        # æ›´é«˜ç²¾åº¦
    vnav_threshold=2.0             # æ›´ä¸¥æ ¼çš„ FAF æ£€æµ‹
)

# é…åˆä¸¥æ ¼éªŒè¯
validation_level = "strict"
enable_duplicate_detection = True
enable_consistency_check = True
```

### è°ƒè¯•é…ç½®ç¤ºä¾‹
```python
# å¼€å‘å’Œè°ƒè¯•ç”¨é…ç½®
config = ConverterConfig(
    output_dir="Debug_Output",
    coordinate_precision=6,         # é™ä½ç²¾åº¦ä»¥åŠ å¿«å¤„ç†
    vnav_threshold=3.0             # å®½æ¾é˜ˆå€¼ä¾¿äºè°ƒè¯•
)

# è°ƒè¯•é€‰é¡¹
debug_config = DebugConfig(debug_mode=True)
save_intermediate_files = True
verbose_logging = True
```

## ğŸ”§ é…ç½®æ–‡ä»¶ç®¡ç†

### é…ç½®æ–‡ä»¶æ ¼å¼
```python
# config.py
from dataclasses import dataclass, asdict
import json

@dataclass 
class TFDIConverterConfig:
    """TFDI è½¬æ¢å™¨å®Œæ•´é…ç½®"""
    # è¾“å‡ºé…ç½®
    output_dir: str = "Primary"
    procedure_legs_dir: str = "Primary/ProcedureLegs"
    archive_name: str = "Primary.7z"
    
    # æ•°æ®å¤„ç†é…ç½®
    coordinate_precision: int = 8
    vnav_threshold: float = 2.5
    
    # æ€§èƒ½é…ç½®
    batch_size: int = 1000
    memory_limit_gb: int = 6
    
    # éªŒè¯é…ç½®
    validation_level: str = "standard"
    enable_coordinate_check: bool = True
    
    # æ—¥å¿—é…ç½®
    log_level: str = "INFO"
    log_file: str = "tfdi_converter.log"
    
    def save_to_file(self, file_path: str):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(asdict(self), f, indent=2)
    
    @classmethod
    def load_from_file(cls, file_path: str):
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return cls(**data)
```

### é…ç½®æ¨¡æ¿
```python
# åˆ›å»ºé…ç½®æ¨¡æ¿
def create_config_templates():
    """åˆ›å»ºå„ç§é…ç½®æ¨¡æ¿"""
    
    # é»˜è®¤é…ç½®
    default_config = TFDIConverterConfig()
    default_config.save_to_file("config_default.json")
    
    # é«˜æ€§èƒ½é…ç½®
    performance_config = TFDIConverterConfig(
        batch_size=2000,
        memory_limit_gb=8,
        coordinate_precision=6
    )
    performance_config.save_to_file("config_performance.json")
    
    # é«˜è´¨é‡é…ç½®
    quality_config = TFDIConverterConfig(
        coordinate_precision=10,
        vnav_threshold=2.0,
        validation_level="strict"
    )
    quality_config.save_to_file("config_quality.json")

# ä½¿ç”¨é…ç½®
config = TFDIConverterConfig.load_from_file("config_performance.json")
```

---

**ä¸‹ä¸€æ­¥**: é…ç½®å®Œæˆåï¼Œè¯·æŸ¥çœ‹ [ä½¿ç”¨è¯´æ˜](usage.md) å¼€å§‹æ‚¨çš„ TFDI æ•°æ®è½¬æ¢ï¼ğŸšâœ¨
