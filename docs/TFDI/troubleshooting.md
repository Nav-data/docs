# ğŸ› ï¸ TFDI å¯¼èˆªæ•°æ®è½¬æ¢å™¨æ•…éšœæ’é™¤

## ğŸš¨ å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

### 1. ç¯å¢ƒå’Œå®‰è£…é—®é¢˜

#### âŒ Python ç¯å¢ƒé—®é¢˜

**é”™è¯¯ä¿¡æ¯ï¼š**
```
ModuleNotFoundError: No module named 'rich'
ImportError: No module named 'pandas'
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# 1. éªŒè¯ Python ç‰ˆæœ¬
python --version  # ç¡®ä¿ â‰¥ 3.8

# 2. å‡çº§ pip
python -m pip install --upgrade pip

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 4. éªŒè¯å®‰è£…
python -c "import rich, pandas; print('ä¾èµ–å®‰è£…æˆåŠŸ')"
```

#### âŒ æƒé™é”™è¯¯

**é”™è¯¯ä¿¡æ¯ï¼š**
```
PermissionError: [Errno 13] Permission denied
æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# Windows - ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ
# å³é”®å‘½ä»¤æç¤ºç¬¦ â†’ "ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ"

# macOS/Linux - ä½¿ç”¨ sudo æˆ–ä¿®æ”¹æƒé™
sudo python converter.py
# æˆ–
chmod 755 /path/to/output/directory
```

### 2. æ•°æ®åº“è®¿é—®é—®é¢˜

#### âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨

**é”™è¯¯ä¿¡æ¯ï¼š**
```
FileNotFoundError: [Errno 2] No such file or directory: 'nd.db3'
æ— æ³•æ‰¾åˆ° Fenix æ•°æ®åº“æ–‡ä»¶
```

**è§£å†³æ–¹æ¡ˆï¼š**
1. **æ£€æŸ¥ Fenix å®‰è£…**ï¼š
   ```bash
   # å¸¸è§è·¯å¾„
   %APPDATA%\Microsoft Flight Simulator\Packages\fenix-a320\
   ```

2. **æ‰‹åŠ¨å®šä½æ–‡ä»¶**ï¼š
   ```bash
   # Windows
   dir /s nd.db3
   
   # macOS/Linux
   find ~ -name "nd.db3" 2>/dev/null
   ```

3. **é‡æ–°ç”Ÿæˆæ•°æ®åº“**ï¼š
   - å¯åŠ¨ MSFS
   - åŠ è½½ Fenix A320
   - ç­‰å¾…å¯¼èˆªæ•°æ®åŠ è½½å®Œæˆ

#### âŒ æ•°æ®åº“æŸå

**é”™è¯¯ä¿¡æ¯ï¼š**
```
sqlite3.DatabaseError: database disk image is malformed
æ•°æ®åº“æ–‡ä»¶å·²æŸå
```

**è¯Šæ–­æ–¹æ³•ï¼š**
```python
import sqlite3

def check_database_integrity(db_path):
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("PRAGMA integrity_check")
        result = cursor.fetchone()
        if result[0] == "ok":
            print("âœ… æ•°æ®åº“å®Œæ•´æ€§æ­£å¸¸")
        else:
            print(f"âŒ æ•°æ®åº“æŸå: {result[0]}")
    except Exception as e:
        print(f"âŒ æ— æ³•è®¿é—®æ•°æ®åº“: {e}")
    finally:
        conn.close()
```

**ä¿®å¤æ–¹æ¡ˆï¼š**
```bash
# 1. å¤‡ä»½æŸåçš„æ•°æ®åº“
cp nd.db3 nd.db3.backup

# 2. å°è¯• SQLite ä¿®å¤
sqlite3 nd.db3 ".dump" | sqlite3 nd_repaired.db3

# 3. å¦‚æœä¿®å¤å¤±è´¥ï¼Œé‡æ–°è·å–æ•°æ®åº“
# åˆ é™¤æ–‡ä»¶å¹¶é‡æ–°å¯åŠ¨ Fenix
```

#### âŒ æ•°æ®åº“è¡¨ç»“æ„ä¸å…¼å®¹

**é”™è¯¯ä¿¡æ¯ï¼š**
```
sqlite3.OperationalError: no such table: Terminals
æ•°æ®åº“ç¼ºå°‘å¿…è¦çš„è¡¨
```

**éªŒè¯è„šæœ¬ï¼š**
```python
def validate_database_schema(db_path):
    required_tables = [
        'Airports', 'Runways', 'Waypoints', 'Navaids',
        'Airways', 'AirwayLegs', 'Terminals', 'TerminalLegs',
        'ILSes', 'AirportLookup', 'NavaidLookup', 'WaypointLookup'
    ]
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
    existing_tables = {row[0] for row in cursor.fetchall()}
    
    missing_tables = set(required_tables) - existing_tables
    if missing_tables:
        print(f"âŒ ç¼ºå°‘è¡¨: {missing_tables}")
        return False
    
    print("âœ… æ•°æ®åº“ç»“æ„éªŒè¯é€šè¿‡")
    return True
```

### 3. å†…å­˜å’Œæ€§èƒ½é—®é¢˜

#### âŒ å†…å­˜ä¸è¶³

**é”™è¯¯ä¿¡æ¯ï¼š**
```
MemoryError: Unable to allocate array
å†…å­˜ä¸è¶³ï¼Œæ— æ³•å¤„ç†æ•°æ®
```

**ç›‘æ§å†…å­˜ä½¿ç”¨ï¼š**
```python
import psutil
import gc

def monitor_memory_usage():
    memory = psutil.virtual_memory()
    print(f"æ€»å†…å­˜: {memory.total // 1024**3} GB")
    print(f"å·²ç”¨å†…å­˜: {memory.used // 1024**3} GB")
    print(f"å¯ç”¨å†…å­˜: {memory.available // 1024**3} GB")
    print(f"ä½¿ç”¨ç‡: {memory.percent}%")

def optimize_memory():
    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()
    
    # æ¸…ç† pandas ç¼“å­˜
    import pandas as pd
    pd.reset_option('all')
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# 1. å‡å°‘æ‰¹å¤„ç†å¤§å°
config = ConverterConfig(
    batch_size=500,  # ä»é»˜è®¤ 1000 å‡å°‘
    coordinate_precision=6  # é™ä½ç²¾åº¦
)

# 2. å¯ç”¨æµå¼å¤„ç†
def process_large_table_streaming(table_name):
    chunk_size = 1000
    offset = 0
    
    while True:
        query = f"""
        SELECT * FROM {table_name} 
        LIMIT {chunk_size} OFFSET {offset}
        """
        
        chunk = pd.read_sql_query(query, conn)
        if chunk.empty:
            break
            
        process_chunk(chunk)
        del chunk  # é‡Šæ”¾å†…å­˜
        gc.collect()
        
        offset += chunk_size
```

#### âŒ å¤„ç†é€Ÿåº¦è¿‡æ…¢

**ç—‡çŠ¶ï¼š** è½¬æ¢è¿‡ç¨‹é•¿æ—¶é—´åœç•™åœ¨æŸä¸ªæ­¥éª¤

**æ€§èƒ½è¯Šæ–­ï¼š**
```python
import time
import cProfile

def profile_conversion():
    profiler = cProfile.Profile()
    profiler.enable()
    
    # æ‰§è¡Œè½¬æ¢
    converter.convert(db_path, terminal_id)
    
    profiler.disable()
    profiler.dump_stats('conversion_profile.prof')

# åˆ†ææ€§èƒ½ç“¶é¢ˆ
# python -m cProfile -o profile.prof converter.py
# python -c "import pstats; pstats.Stats('profile.prof').sort_stats('cumulative').print_stats(10)"
```

**ä¼˜åŒ–å»ºè®®ï¼š**
```python
# 1. SQLite æ€§èƒ½ä¼˜åŒ–
def optimize_sqlite_connection(conn):
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.execute("PRAGMA cache_size=10000")
    conn.execute("PRAGMA temp_store=MEMORY")

# 2. å¹¶è¡Œå¤„ç†
from concurrent.futures import ThreadPoolExecutor

def parallel_table_processing():
    tables = ['Airports', 'Runways', 'Waypoints', 'Navaids']
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = []
        for table in tables:
            future = executor.submit(process_table, table)
            futures.append(future)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for future in futures:
            future.result()
```

### 4. æ•°æ®è½¬æ¢é—®é¢˜

#### âŒ åæ ‡æ•°æ®å¼‚å¸¸

**é”™è¯¯ä¿¡æ¯ï¼š**
```
ValueError: Invalid coordinate value: latitude=91.5
åæ ‡è¶…å‡ºæœ‰æ•ˆèŒƒå›´
```

**éªŒè¯å’Œä¿®å¤ï¼š**
```python
def validate_and_fix_coordinates(df):
    """éªŒè¯å’Œä¿®å¤åæ ‡æ•°æ®"""
    initial_count = len(df)
    
    # æ£€æŸ¥çº¬åº¦èŒƒå›´ [-90, 90]
    invalid_lat = (df['Latitude'] < -90) | (df['Latitude'] > 90)
    if invalid_lat.any():
        print(f"å‘ç° {invalid_lat.sum()} ä¸ªæ— æ•ˆçº¬åº¦å€¼")
        df = df[~invalid_lat]
    
    # æ£€æŸ¥ç»åº¦èŒƒå›´ [-180, 180]
    invalid_lon = (df['Longitude'] < -180) | (df['Longitude'] > 180)
    if invalid_lon.any():
        print(f"å‘ç° {invalid_lon.sum()} ä¸ªæ— æ•ˆç»åº¦å€¼")
        df = df[~invalid_lon]
    
    removed_count = initial_count - len(df)
    if removed_count > 0:
        print(f"âš ï¸ ç§»é™¤äº† {removed_count} ä¸ªæ— æ•ˆåæ ‡è®°å½•")
    
    return df
```

#### âŒ JSON åºåˆ—åŒ–å¤±è´¥

**é”™è¯¯ä¿¡æ¯ï¼š**
```
TypeError: Object of type 'datetime' is not JSON serializable
JSON åºåˆ—åŒ–é”™è¯¯
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
import json
from datetime import datetime
import numpy as np

class CustomJSONEncoder(json.JSONEncoder):
    """è‡ªå®šä¹‰ JSON ç¼–ç å™¨"""
    
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super().default(obj)

# ä½¿ç”¨è‡ªå®šä¹‰ç¼–ç å™¨
def safe_json_dump(data, file_path):
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, cls=CustomJSONEncoder, 
                 ensure_ascii=False, indent=2)
```

#### âŒ å­—ç¬¦ç¼–ç é—®é¢˜

**é”™è¯¯ä¿¡æ¯ï¼š**
```
UnicodeDecodeError: 'utf-8' codec can't decode byte
å­—ç¬¦ç¼–ç é”™è¯¯
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
import chardet

def detect_and_convert_encoding(file_path):
    """æ£€æµ‹å¹¶è½¬æ¢æ–‡ä»¶ç¼–ç """
    # æ£€æµ‹ç¼–ç 
    with open(file_path, 'rb') as f:
        raw_data = f.read()
        encoding = chardet.detect(raw_data)['encoding']
    
    print(f"æ£€æµ‹åˆ°ç¼–ç : {encoding}")
    
    # è½¬æ¢ä¸º UTF-8
    with open(file_path, 'r', encoding=encoding) as f:
        content = f.read()
    
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)

def safe_string_handling(text):
    """å®‰å…¨çš„å­—ç¬¦ä¸²å¤„ç†"""
    if isinstance(text, bytes):
        # å°è¯•å¤šç§ç¼–ç 
        for encoding in ['utf-8', 'gbk', 'latin1']:
            try:
                return text.decode(encoding)
            except UnicodeDecodeError:
                continue
        # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨é”™è¯¯å¤„ç†
        return text.decode('utf-8', errors='replace')
    return str(text)
```

### 5. è¾“å‡ºæ–‡ä»¶é—®é¢˜

#### âŒ å‹ç¼©åŒ…åˆ›å»ºå¤±è´¥

**é”™è¯¯ä¿¡æ¯ï¼š**
```
py7zr.exceptions.Bad7zFile: not a 7z file
å‹ç¼©åŒ…åˆ›å»ºå¤±è´¥
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
import py7zr
import shutil
from pathlib import Path

def safe_create_archive(source_dir, archive_path):
    """å®‰å…¨åˆ›å»ºå‹ç¼©åŒ…"""
    try:
        # ç¡®ä¿æºç›®å½•å­˜åœ¨
        if not Path(source_dir).exists():
            raise FileNotFoundError(f"æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
        
        # åˆ é™¤å·²å­˜åœ¨çš„å‹ç¼©åŒ…
        if Path(archive_path).exists():
            Path(archive_path).unlink()
        
        # åˆ›å»ºå‹ç¼©åŒ…
        with py7zr.SevenZipFile(archive_path, 'w') as archive:
            archive.writeall(source_dir, ".")
        
        print(f"âœ… å‹ç¼©åŒ…åˆ›å»ºæˆåŠŸ: {archive_path}")
        return True
        
    except Exception as e:
        print(f"âŒ å‹ç¼©åŒ…åˆ›å»ºå¤±è´¥: {e}")
        
        # å›é€€æ–¹æ¡ˆï¼šåˆ›å»º ZIP æ–‡ä»¶
        try:
            shutil.make_archive(
                str(Path(archive_path).with_suffix('')), 
                'zip', 
                source_dir
            )
            print("âœ… å·²åˆ›å»º ZIP æ ¼å¼å¤‡ç”¨æ–‡ä»¶")
            return True
        except Exception as zip_error:
            print(f"âŒ ZIP åˆ›å»ºä¹Ÿå¤±è´¥: {zip_error}")
            return False
```

#### âŒ æ–‡ä»¶å¤§å°å¼‚å¸¸

**ç—‡çŠ¶ï¼š** è¾“å‡ºæ–‡ä»¶è¿‡å°æˆ–è¿‡å¤§

**æ£€æŸ¥æ–¹æ³•ï¼š**
```python
def validate_output_files(output_dir):
    """éªŒè¯è¾“å‡ºæ–‡ä»¶"""
    expected_files = [
        'Airports.json', 'Runways.json', 'Waypoints.json',
        'Navaids.json', 'Airways.json', 'AirwayLegs.json',
        'Terminals.json', 'ILSes.json'
    ]
    
    file_info = {}
    for file_name in expected_files:
        file_path = Path(output_dir) / file_name
        if file_path.exists():
            size = file_path.stat().st_size
            file_info[file_name] = {
                'exists': True,
                'size_mb': size / 1024 / 1024,
                'empty': size == 0
            }
        else:
            file_info[file_name] = {'exists': False}
    
    # æ‰“å°æ–‡ä»¶ä¿¡æ¯
    print("ğŸ“ è¾“å‡ºæ–‡ä»¶æ£€æŸ¥:")
    for file_name, info in file_info.items():
        if info['exists']:
            if info.get('empty', False):
                print(f"âš ï¸ {file_name}: ç©ºæ–‡ä»¶")
            else:
                print(f"âœ… {file_name}: {info['size_mb']:.2f} MB")
        else:
            print(f"âŒ {file_name}: æ–‡ä»¶ç¼ºå¤±")
    
    return file_info
```

## ğŸ” è¯Šæ–­å·¥å…·

### 1. ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥

```python
def system_diagnostics():
    """ç³»ç»Ÿç¯å¢ƒè¯Šæ–­"""
    import platform
    import sys
    import psutil
    
    print("ğŸ” ç³»ç»Ÿç¯å¢ƒè¯Šæ–­")
    print("=" * 50)
    
    # æ“ä½œç³»ç»Ÿä¿¡æ¯
    print(f"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    print(f"æ¶æ„: {platform.machine()}")
    
    # Python ç¯å¢ƒ
    print(f"Python ç‰ˆæœ¬: {sys.version}")
    print(f"Python è·¯å¾„: {sys.executable}")
    
    # ç¡¬ä»¶ä¿¡æ¯
    print(f"CPU æ ¸å¿ƒæ•°: {psutil.cpu_count()}")
    memory = psutil.virtual_memory()
    print(f"æ€»å†…å­˜: {memory.total // 1024**3} GB")
    print(f"å¯ç”¨å†…å­˜: {memory.available // 1024**3} GB")
    
    # ç£ç›˜ç©ºé—´
    disk = psutil.disk_usage('.')
    print(f"ç£ç›˜æ€»ç©ºé—´: {disk.total // 1024**3} GB")
    print(f"ç£ç›˜å¯ç”¨ç©ºé—´: {disk.free // 1024**3} GB")
```

### 2. ä¾èµ–åŒ…éªŒè¯

```python
def verify_dependencies():
    """éªŒè¯ä¾èµ–åŒ…"""
    required_packages = [
        'rich', 'pandas', 'py7zr', 'sqlite3'
    ]
    
    print("ğŸ“¦ ä¾èµ–åŒ…éªŒè¯")
    print("=" * 30)
    
    for package in required_packages:
        try:
            module = __import__(package)
            version = getattr(module, '__version__', 'Unknown')
            print(f"âœ… {package}: {version}")
        except ImportError:
            print(f"âŒ {package}: æœªå®‰è£…")
        except Exception as e:
            print(f"âš ï¸ {package}: é”™è¯¯ - {e}")
```

### 3. æ€§èƒ½ç›‘æ§å·¥å…·

```python
import time
import threading
from contextlib import contextmanager

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {}
        self.monitoring = False
    
    @contextmanager
    def measure(self, operation_name):
        """æµ‹é‡æ“ä½œè€—æ—¶"""
        start_time = time.time()
        start_memory = psutil.virtual_memory().used
        
        try:
            yield
        finally:
            end_time = time.time()
            end_memory = psutil.virtual_memory().used
            
            self.metrics[operation_name] = {
                'duration': end_time - start_time,
                'memory_delta': end_memory - start_memory
            }
    
    def start_monitoring(self):
        """å¼€å§‹å®æ—¶ç›‘æ§"""
        self.monitoring = True
        
        def monitor():
            while self.monitoring:
                cpu_percent = psutil.cpu_percent()
                memory = psutil.virtual_memory()
                
                print(f"\rğŸ”„ CPU: {cpu_percent:5.1f}% | "
                      f"å†…å­˜: {memory.percent:5.1f}% | "
                      f"å¯ç”¨: {memory.available//1024**2:,} MB", 
                      end='', flush=True)
                
                time.sleep(1)
        
        monitor_thread = threading.Thread(target=monitor, daemon=True)
        monitor_thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        print()  # æ¢è¡Œ
    
    def print_summary(self):
        """æ‰“å°æ€§èƒ½æ‘˜è¦"""
        print("\nğŸ“Š æ€§èƒ½æ‘˜è¦:")
        print("-" * 40)
        
        for operation, metrics in self.metrics.items():
            duration = metrics['duration']
            memory_mb = metrics['memory_delta'] / 1024 / 1024
            
            print(f"{operation:20s}: {duration:8.2f}s | "
                  f"{memory_mb:+8.1f}MB")

# ä½¿ç”¨ç¤ºä¾‹
monitor = PerformanceMonitor()
monitor.start_monitoring()

with monitor.measure("æ•°æ®åº“éªŒè¯"):
    validate_database(db_path)

with monitor.measure("æ•°æ®è½¬æ¢"):
    convert_data()

monitor.stop_monitoring()
monitor.print_summary()
```

## ğŸ“‹ æ•…éšœæ’é™¤æ¸…å•

### ğŸ”§ é¢„è½¬æ¢æ£€æŸ¥
- [ ] Python ç‰ˆæœ¬ â‰¥ 3.8
- [ ] æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…ä¸”ç‰ˆæœ¬æ­£ç¡®
- [ ] Fenix æ•°æ®åº“æ–‡ä»¶å­˜åœ¨ä¸”å®Œæ•´
- [ ] è¶³å¤Ÿçš„å¯ç”¨å†…å­˜ (æ¨è 4GB+)
- [ ] è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ (æ¨è 1GB+)
- [ ] è¾“å‡ºç›®å½•æœ‰å†™å…¥æƒé™

### ğŸ“Š è½¬æ¢è¿‡ç¨‹æ£€æŸ¥
- [ ] æ•°æ®åº“è¿æ¥æˆåŠŸ
- [ ] æ‰€æœ‰å¿…éœ€è¡¨éƒ½å­˜åœ¨
- [ ] åæ ‡æ•°æ®åœ¨æœ‰æ•ˆèŒƒå›´å†…
- [ ] å†…å­˜ä½¿ç”¨åœ¨åˆç†èŒƒå›´å†…
- [ ] æ²¡æœ‰å‡ºç°æƒé™é”™è¯¯
- [ ] ä¸´æ—¶æ–‡ä»¶æ­£å¸¸åˆ›å»º

### ğŸ“ åè½¬æ¢éªŒè¯
- [ ] æ‰€æœ‰ JSON æ–‡ä»¶å·²ç”Ÿæˆ
- [ ] æ–‡ä»¶å¤§å°åˆç†ï¼ˆä¸ä¸ºç©ºæˆ–å¼‚å¸¸å¤§ï¼‰
- [ ] JSON æ ¼å¼æœ‰æ•ˆ
- [ ] å‹ç¼©åŒ…åˆ›å»ºæˆåŠŸ
- [ ] ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†
- [ ] æ—¥å¿—æ— ä¸¥é‡é”™è¯¯

## ğŸ†˜ è·å–å¸®åŠ©

### è‡ªåŠ©è¯Šæ–­
1. **è¿è¡Œè¯Šæ–­å·¥å…·**ï¼š
   ```python
   from tfdi_converter.diagnostics import run_full_diagnostics
   run_full_diagnostics()
   ```

2. **æŸ¥çœ‹è¯¦ç»†æ—¥å¿—**ï¼š
   ```bash
   tail -f converter.log
   grep -i error converter.log
   ```

3. **æ£€æŸ¥ç³»ç»Ÿèµ„æº**ï¼š
   ```bash
   # Windows
   taskmgr
   
   # macOS
   activity monitor
   
   # Linux
   top
   htop
   ```

### ç¤¾åŒºæ”¯æŒ
- **GitHub Issues**: æŠ¥å‘Š Bug å’ŒæŠ€æœ¯é—®é¢˜
- **GitHub Discussions**: ä½¿ç”¨é—®é¢˜å’Œç»éªŒåˆ†äº«
- **é¡¹ç›®æ–‡æ¡£**: æŸ¥é˜…å®Œæ•´ä½¿ç”¨æŒ‡å—

### æŠ¥å‘Šé—®é¢˜æ—¶è¯·æä¾›ï¼š
- **å®Œæ•´é”™è¯¯æ—¥å¿—**
- **ç³»ç»Ÿç¯å¢ƒä¿¡æ¯**
- **è½¬æ¢å™¨ç‰ˆæœ¬**
- **æ•°æ®åº“ä¿¡æ¯**ï¼ˆå¤§å°ã€AIRAC ç­‰ï¼‰
- **é‡ç°æ­¥éª¤**
- **ç›¸å…³é…ç½®æ–‡ä»¶**

---

**é‡åˆ°æœªè§£å†³çš„é—®é¢˜ï¼Ÿ** 

è¯·åœ¨ [GitHub Issues](https://github.com/your-org/tfdi-converter/issues) ä¸­åˆ›å»ºæ–°é—®é¢˜ï¼Œæˆ‘ä»¬ä¼šå°½å¿«ååŠ©è§£å†³ï¼ğŸšâœ¨
