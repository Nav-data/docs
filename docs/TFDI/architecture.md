# ğŸ—ï¸ TFDI å¯¼èˆªæ•°æ®è½¬æ¢å™¨æ¶æ„

## ç³»ç»Ÿæ¦‚è§ˆ

TFDI å¯¼èˆªæ•°æ®è½¬æ¢å™¨æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èˆªç©ºå¯¼èˆªæ•°æ®è½¬æ¢å·¥å…·ï¼Œä¸“é—¨è®¾è®¡ç”¨äºå°† Fenix A320 å¯¼èˆªæ•°æ®åº“è½¬æ¢ä¸º TFDI MD-11 å…¼å®¹çš„ JSON æ ¼å¼ã€‚è¯¥å·¥å…·é‡‡ç”¨ç°ä»£åŒ–çš„æ¶æ„è®¾è®¡ï¼Œæä¾›é«˜æ•ˆã€å¯é çš„æ•°æ®è½¬æ¢æœåŠ¡ã€‚

## ğŸ¯ è®¾è®¡åŸåˆ™

### 1. æ•°æ®å®Œæ•´æ€§ä¼˜å…ˆ
- **ä¸¥æ ¼éªŒè¯**ï¼šå¤šå±‚æ•°æ®éªŒè¯æœºåˆ¶
- **å…³ç³»ä¿æŒ**ï¼šç»´æŠ¤å¯¼èˆªæ•°æ®ä¹‹é—´çš„ä¾èµ–å…³ç³»
- **ç²¾åº¦ä¿è¯**ï¼šä¿æŒåæ ‡å’Œè®¡ç®—çš„é«˜ç²¾åº¦
- **ä¸€è‡´æ€§æ£€æŸ¥**ï¼šç¡®ä¿è½¬æ¢åæ•°æ®çš„é€»è¾‘ä¸€è‡´æ€§

### 2. æ€§èƒ½ä¼˜åŒ–å¯¼å‘
- **SQLite ä¼˜åŒ–**ï¼šWAL æ¨¡å¼å’Œæ€§èƒ½è°ƒä¼˜
- **æ‰¹é‡å¤„ç†**ï¼šå†…å­˜é«˜æ•ˆçš„æ‰¹å¤„ç†ç­–ç•¥
- **ç¼“å­˜æœºåˆ¶**ï¼šæ™ºèƒ½æ•°æ®ç¼“å­˜å’Œå¤ç”¨
- **å‹ç¼©ä¼˜åŒ–**ï¼šå¿«é€Ÿ 7z å‹ç¼©å’Œæ¸…ç†

### 3. ç”¨æˆ·ä½“éªŒè‡³ä¸Š
- **Rich CLI**ï¼šç°ä»£åŒ–å½©è‰²ç»ˆç«¯ç•Œé¢
- **å®æ—¶åé¦ˆ**ï¼šè¯¦ç»†çš„è¿›åº¦æ˜¾ç¤ºå’ŒçŠ¶æ€æ›´æ–°
- **å‹å¥½æç¤º**ï¼šä¸“ä¸šçš„é”™è¯¯å¤„ç†å’Œæ¢å¤å»ºè®®
- **äº¤äº’è®¾è®¡**ï¼šç›´è§‚çš„æ“ä½œæµç¨‹å¼•å¯¼

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    A[ç”¨æˆ·ç•Œé¢å±‚] --> B[ä¸šåŠ¡é€»è¾‘å±‚]
    B --> C[æ•°æ®å¤„ç†å±‚]
    C --> D[å­˜å‚¨è®¿é—®å±‚]
    
    A --> A1[Rich CLI ç•Œé¢]
    A --> A2[è¿›åº¦ç®¡ç†å™¨]
    A --> A3[ç”¨æˆ·äº¤äº’]
    
    B --> B1[è½¬æ¢æ§åˆ¶å™¨]
    B --> B2[é…ç½®ç®¡ç†å™¨]
    B --> B3[éªŒè¯å¼•æ“]
    B --> B4[FAF æ£€æµ‹å™¨]
    
    C --> C1[SQLite å¤„ç†å™¨]
    C --> C2[åæ ‡æ ‡å‡†åŒ–å™¨]
    C --> C3[JSON åºåˆ—åŒ–å™¨]
    C --> C4[å‹ç¼©ç®¡ç†å™¨]
    
    D --> D1[Fenix æ•°æ®åº“]
    D --> D2[JSON æ–‡ä»¶]
    D --> D3[7z å‹ç¼©åŒ…]
```

### æ ¸å¿ƒç»„ä»¶è¯¦è§£

#### 1. ç”¨æˆ·ç•Œé¢å±‚ (UI Layer)
**èŒè´£**: æä¾›ç”¨æˆ·äº¤äº’ç•Œé¢å’Œåé¦ˆ
```python
class RichInterface:
    """Rich CLI ç•Œé¢ç®¡ç†å™¨"""
    - progress_tracking: è¿›åº¦æ¡ç®¡ç†
    - status_display: çŠ¶æ€ä¿¡æ¯æ˜¾ç¤º
    - error_presentation: é”™è¯¯ä¿¡æ¯å±•ç¤º
    - user_input: ç”¨æˆ·è¾“å…¥å¤„ç†
```

#### 2. ä¸šåŠ¡é€»è¾‘å±‚ (Business Layer)
**èŒè´£**: æ ¸å¿ƒä¸šåŠ¡é€»è¾‘å’Œæµç¨‹æ§åˆ¶
```python
class FenixToTFDIConverter:
    """ä¸»è½¬æ¢å™¨ç±»"""
    - database_validation: æ•°æ®åº“éªŒè¯
    - conversion_orchestration: è½¬æ¢æµç¨‹ç¼–æ’
    - faf_detection: FAF ç‚¹æ£€æµ‹
    - data_normalization: æ•°æ®æ ‡å‡†åŒ–
```

#### 3. æ•°æ®å¤„ç†å±‚ (Data Layer)
**èŒè´£**: æ•°æ®è½¬æ¢å’Œå¤„ç†ç®—æ³•
```python
class DataProcessor:
    """æ•°æ®å¤„ç†æ ¸å¿ƒ"""
    - coordinate_precision: åæ ‡ç²¾åº¦å¤„ç†
    - column_standardization: åˆ—åæ ‡å‡†åŒ–
    - relationship_mapping: å…³ç³»æ˜ å°„
    - format_conversion: æ ¼å¼è½¬æ¢
```

#### 4. å­˜å‚¨è®¿é—®å±‚ (Storage Layer)
**èŒè´£**: æ•°æ®åº“è®¿é—®å’Œæ–‡ä»¶æ“ä½œ
```python
class StorageManager:
    """å­˜å‚¨ç®¡ç†å™¨"""
    - sqlite_optimization: SQLite æ€§èƒ½ä¼˜åŒ–
    - file_operations: æ–‡ä»¶è¯»å†™æ“ä½œ
    - compression_handling: å‹ç¼©æ–‡ä»¶å¤„ç†
    - backup_management: å¤‡ä»½ç®¡ç†
```

## ğŸ“Š æ•°æ®æµæ¶æ„

### è½¬æ¢æµæ°´çº¿

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant UI as ç•Œé¢å±‚
    participant BL as ä¸šåŠ¡å±‚
    participant DL as æ•°æ®å±‚
    participant SL as å­˜å‚¨å±‚
    
    U->>UI: å¯åŠ¨è½¬æ¢
    UI->>BL: åˆå§‹åŒ–è½¬æ¢å™¨
    BL->>SL: éªŒè¯æ•°æ®åº“
    SL->>BL: è¿”å›éªŒè¯ç»“æœ
    BL->>DL: å¼€å§‹æ•°æ®å¤„ç†
    
    loop æ¯ä¸ªæ•°æ®è¡¨
        DL->>SL: è¯»å–æ•°æ®
        DL->>DL: æ ‡å‡†åŒ–å¤„ç†
        DL->>DL: åæ ‡ç²¾åº¦è°ƒæ•´
        DL->>SL: å†™å…¥ JSON
        DL->>UI: æ›´æ–°è¿›åº¦
    end
    
    DL->>BL: å¤„ç†å®Œæˆ
    BL->>SL: åˆ›å»ºå‹ç¼©åŒ…
    SL->>UI: è¿”å›ç»“æœ
    UI->>U: æ˜¾ç¤ºå®ŒæˆçŠ¶æ€
```

### æ•°æ®æ˜ å°„æ¶æ„

```mermaid
graph LR
    A[Fenix SQLite æ•°æ®åº“] --> B[æ•°æ®æå–å±‚]
    B --> C[æ ‡å‡†åŒ–å±‚]
    C --> D[éªŒè¯å±‚]
    D --> E[è½¬æ¢å±‚]
    E --> F[åºåˆ—åŒ–å±‚]
    F --> G[JSON æ–‡ä»¶é›†]
    G --> H[å‹ç¼©å±‚]
    H --> I[TFDI å…¼å®¹åŒ…]
```

## ğŸ”§ æŠ€æœ¯æ ˆ

### æ ¸å¿ƒæŠ€æœ¯

| ç»„ä»¶ | æŠ€æœ¯é€‰å‹ | ç‰ˆæœ¬è¦æ±‚ | ç”¨é€” |
|------|----------|----------|------|
| **Python** | Python 3.8+ | â‰¥ 3.8.0 | ä¸»è¦ç¼–ç¨‹è¯­è¨€ |
| **Rich** | Rich Library | â‰¥ 12.0.0 | CLI ç•Œé¢ç¾åŒ– |
| **SQLite3** | å†…ç½®æ¨¡å— | Python å†…ç½® | æ•°æ®åº“è®¿é—® |
| **Pandas** | DataFrame | â‰¥ 1.3.0 | æ•°æ®å¤„ç† |
| **JSON** | å†…ç½®æ¨¡å— | Python å†…ç½® | æ•°æ®åºåˆ—åŒ– |
| **py7zr** | 7-Zip Python | â‰¥ 0.18.0 | å‹ç¼©å¤„ç† |

### æ¶æ„ç‰¹å¾

#### 1. æ¨¡å—åŒ–è®¾è®¡
```python
fenix_to_tfdi/
â”œâ”€â”€ core/                  # æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ converter.py       # ä¸»è½¬æ¢å™¨
â”‚   â”œâ”€â”€ validator.py       # æ•°æ®éªŒè¯å™¨
â”‚   â””â”€â”€ config.py         # é…ç½®ç®¡ç†
â”œâ”€â”€ data/                  # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ processor.py       # æ•°æ®å¤„ç†å™¨
â”‚   â”œâ”€â”€ normalizer.py      # æ ‡å‡†åŒ–å·¥å…·
â”‚   â””â”€â”€ serializer.py     # åºåˆ—åŒ–å·¥å…·
â”œâ”€â”€ ui/                    # ç”¨æˆ·ç•Œé¢
â”‚   â”œâ”€â”€ cli.py            # å‘½ä»¤è¡Œç•Œé¢
â”‚   â””â”€â”€ progress.py       # è¿›åº¦ç®¡ç†
â””â”€â”€ utils/                 # å·¥å…·æ¨¡å—
    â”œâ”€â”€ storage.py        # å­˜å‚¨å·¥å…·
    â””â”€â”€ compression.py    # å‹ç¼©å·¥å…·
```

#### 2. é…ç½®é©±åŠ¨æ¶æ„
```python
@dataclass
class ConverterConfig:
    """è½¬æ¢å™¨é…ç½®ç±»"""
    output_dir: str = "Primary"
    procedure_legs_dir: str = "Primary/ProcedureLegs"
    archive_name: str = "Primary.7z"
    coordinate_precision: int = 8
    vnav_threshold: float = 2.5
    
    # SQLite ä¼˜åŒ–é…ç½®
    sqlite_pragmas: Dict[str, str] = field(default_factory=lambda: {
        "journal_mode": "WAL",
        "synchronous": "NORMAL",
        "cache_size": "10000",
        "temp_store": "MEMORY"
    })
```

## ğŸš€ æ€§èƒ½æ¶æ„

### å†…å­˜ç®¡ç†ç­–ç•¥

#### 1. æµå¼å¤„ç†
```python
def process_large_table(table_name: str, batch_size: int = 1000):
    """æµå¼å¤„ç†å¤§è¡¨æ•°æ®"""
    offset = 0
    while True:
        query = f"""
        SELECT * FROM {table_name} 
        LIMIT {batch_size} OFFSET {offset}
        """
        
        batch = execute_query(query)
        if not batch:
            break
            
        process_batch(batch)
        offset += batch_size
```

#### 2. ç¼“å­˜ä¼˜åŒ–
```python
class WaypointCache:
    """èˆªè·¯ç‚¹ç¼“å­˜ç®¡ç†"""
    def __init__(self, max_size: int = 10000):
        self._cache: Dict[str, WaypointData] = {}
        self._max_size = max_size
        self._access_times: Dict[str, float] = {}
    
    def get_waypoint(self, waypoint_id: str) -> Optional[WaypointData]:
        """è·å–ç¼“å­˜çš„èˆªè·¯ç‚¹æ•°æ®"""
        if waypoint_id in self._cache:
            self._access_times[waypoint_id] = time.time()
            return self._cache[waypoint_id]
        return None
```

### å¹¶å‘å¤„ç†æ¶æ„

#### 1. å¤šçº¿ç¨‹è®¾è®¡
```python
class ConcurrentProcessor:
    """å¹¶å‘å¤„ç†å™¨"""
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    def process_tables_parallel(self, tables: List[str]):
        """å¹¶è¡Œå¤„ç†å¤šä¸ªè¡¨"""
        futures = []
        for table in tables:
            future = self.executor.submit(self.process_table, table)
            futures.append(future)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        concurrent.futures.wait(futures)
```

#### 2. èµ„æºæ± ç®¡ç†
```python
class DatabaseConnectionPool:
    """æ•°æ®åº“è¿æ¥æ± """
    def __init__(self, db_path: str, pool_size: int = 5):
        self.db_path = db_path
        self.pool_size = pool_size
        self.connections: Queue = Queue(maxsize=pool_size)
        self._init_pool()
    
    def get_connection(self) -> sqlite3.Connection:
        """è·å–æ•°æ®åº“è¿æ¥"""
        return self.connections.get()
    
    def return_connection(self, conn: sqlite3.Connection):
        """å½’è¿˜æ•°æ®åº“è¿æ¥"""
        self.connections.put(conn)
```

## ğŸ”’ å®‰å…¨æ¶æ„

### æ•°æ®ä¿æŠ¤æœºåˆ¶

#### 1. è¾“å…¥éªŒè¯
```python
class InputValidator:
    """è¾“å…¥éªŒè¯å™¨"""
    
    @staticmethod
    def validate_database_path(path: str) -> bool:
        """éªŒè¯æ•°æ®åº“è·¯å¾„å®‰å…¨æ€§"""
        # æ£€æŸ¥è·¯å¾„éå†æ”»å‡»
        if ".." in path or path.startswith("/"):
            return False
        
        # éªŒè¯æ–‡ä»¶æ‰©å±•å
        if not path.endswith(('.db', '.db3', '.sqlite')):
            return False
        
        return True
    
    @staticmethod  
    def validate_terminal_id(terminal_id: int) -> bool:
        """éªŒè¯ç»ˆç«¯IDèŒƒå›´"""
        return 1 <= terminal_id <= 999999
```

#### 2. é”™è¯¯éš”ç¦»
```python
class SafeConverter:
    """å®‰å…¨è½¬æ¢å™¨"""
    
    def safe_convert_table(self, table_name: str) -> bool:
        """å®‰å…¨çš„è¡¨è½¬æ¢"""
        try:
            with self.create_transaction() as transaction:
                result = self.convert_table(table_name)
                transaction.commit()
                return result
        except DatabaseError as e:
            self.logger.error(f"æ•°æ®åº“é”™è¯¯: {e}")
            transaction.rollback()
            return False
        except Exception as e:
            self.logger.error(f"æœªçŸ¥é”™è¯¯: {e}")
            return False
```

## ğŸ“ˆ å¯æ‰©å±•æ¶æ„

### æ’ä»¶ç³»ç»Ÿè®¾è®¡

#### 1. è½¬æ¢å™¨æ’ä»¶æ¥å£
```python
class ConverterPlugin(ABC):
    """è½¬æ¢å™¨æ’ä»¶æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def get_name(self) -> str:
        """è·å–æ’ä»¶åç§°"""
        pass
    
    @abstractmethod
    def get_supported_formats(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ ¼å¼"""
        pass
    
    @abstractmethod
    def convert_data(self, data: Any, config: ConverterConfig) -> Any:
        """è½¬æ¢æ•°æ®"""
        pass
```

#### 2. æ ¼å¼æ‰©å±•æœºåˆ¶
```python
class FormatRegistry:
    """æ ¼å¼æ³¨å†Œè¡¨"""
    
    def __init__(self):
        self._converters: Dict[str, ConverterPlugin] = {}
    
    def register_converter(self, format_name: str, converter: ConverterPlugin):
        """æ³¨å†Œè½¬æ¢å™¨"""
        self._converters[format_name] = converter
    
    def get_converter(self, format_name: str) -> Optional[ConverterPlugin]:
        """è·å–è½¬æ¢å™¨"""
        return self._converters.get(format_name)
```

### æ•°æ®æºæ‰©å±•

#### 1. æ•°æ®æºæŠ½è±¡
```python
class DataSource(ABC):
    """æ•°æ®æºæŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def connect(self) -> bool:
        """è¿æ¥æ•°æ®æº"""
        pass
    
    @abstractmethod
    def get_tables(self) -> List[str]:
        """è·å–è¡¨åˆ—è¡¨"""
        pass
    
    @abstractmethod
    def query_data(self, query: str) -> Iterator[Dict]:
        """æŸ¥è¯¢æ•°æ®"""
        pass
```

## ğŸ”„ ç»´æŠ¤æ€§æ¶æ„

### æ—¥å¿—è®°å½•ç³»ç»Ÿ

#### 1. ç»“æ„åŒ–æ—¥å¿—
```python
class StructuredLogger:
    """ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)
        
        # Rich æ ¼å¼åŒ–å¤„ç†å™¨
        rich_handler = RichHandler(rich_tracebacks=True)
        rich_handler.setFormatter(
            logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
        )
        self.logger.addHandler(rich_handler)
    
    def log_conversion_start(self, table_name: str, record_count: int):
        """è®°å½•è½¬æ¢å¼€å§‹"""
        self.logger.info(
            f"å¼€å§‹è½¬æ¢è¡¨ {table_name}",
            extra={
                "table": table_name,
                "record_count": record_count,
                "operation": "conversion_start"
            }
        )
```

#### 2. æ€§èƒ½ç›‘æ§
```python
class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics: Dict[str, List[float]] = defaultdict(list)
    
    @contextmanager
    def measure_time(self, operation: str):
        """æµ‹é‡æ“ä½œè€—æ—¶"""
        start_time = time.time()
        try:
            yield
        finally:
            elapsed = time.time() - start_time
            self.metrics[operation].append(elapsed)
            self.logger.debug(f"{operation} è€—æ—¶: {elapsed:.2f}s")
```

## ğŸ“Š æµ‹è¯•æ¶æ„

### æµ‹è¯•ç­–ç•¥

#### 1. åˆ†å±‚æµ‹è¯•
```python
# å•å…ƒæµ‹è¯•
class TestDataProcessor(unittest.TestCase):
    def test_coordinate_normalization(self):
        """æµ‹è¯•åæ ‡æ ‡å‡†åŒ–"""
        processor = DataProcessor()
        result = processor.normalize_coordinate(39.916667, 8)
        self.assertEqual(result, 39.91666700)

# é›†æˆæµ‹è¯•  
class TestConverterIntegration(unittest.TestCase):
    def test_full_conversion_pipeline(self):
        """æµ‹è¯•å®Œæ•´è½¬æ¢æµæ°´çº¿"""
        converter = FenixToTFDIConverter(test_config)
        result = converter.convert(test_database_path)
        self.assertTrue(result)

# æ€§èƒ½æµ‹è¯•
class TestPerformance(unittest.TestCase):
    def test_large_database_conversion(self):
        """æµ‹è¯•å¤§å‹æ•°æ®åº“è½¬æ¢æ€§èƒ½"""
        start_time = time.time()
        converter.convert(large_test_database)
        elapsed = time.time() - start_time
        self.assertLess(elapsed, 300)  # åº”åœ¨5åˆ†é’Ÿå†…å®Œæˆ
```

---

è¿™ä¸ªæ¶æ„è®¾è®¡ç¡®ä¿äº† TFDI å¯¼èˆªæ•°æ®è½¬æ¢å™¨çš„**å¯é æ€§**ã€**æ€§èƒ½**å’Œ**å¯ç»´æŠ¤æ€§**ï¼Œä¸º TFDI MD-11 é£è¡Œæ¨¡æ‹Ÿç¤¾åŒºæä¾›ä¸“ä¸šçº§çš„æ•°æ®è½¬æ¢è§£å†³æ–¹æ¡ˆã€‚ğŸšâœ¨
